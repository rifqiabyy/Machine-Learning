{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "softmax regresion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkTbQMlM/oFLjxYQxA8DHI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rifqiabyy/Machine-Learning/blob/main/week%209%20ML/softmax_regresion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBQuesVjFO6k",
        "outputId": "63e5f437-aff8-4e67-d8a5-097fe33cf4b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: d2l in /usr/local/lib/python3.7/dist-packages (0.17.5)\n",
            "Requirement already satisfied: pandas==1.2.4 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.2.4)\n",
            "Requirement already satisfied: requests==2.25.1 in /usr/local/lib/python3.7/dist-packages (from d2l) (2.25.1)\n",
            "Requirement already satisfied: numpy==1.21.5 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.21.5)\n",
            "Requirement already satisfied: matplotlib==3.5.1 in /usr/local/lib/python3.7/dist-packages (from d2l) (3.5.1)\n",
            "Requirement already satisfied: jupyter==1.0.0 in /usr/local/lib/python3.7/dist-packages (from d2l) (1.0.0)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (4.10.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.6.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.3.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (5.2.0)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.7/dist-packages (from jupyter==1.0.0->d2l) (7.7.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (1.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (21.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (0.11.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (3.0.9)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (4.33.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.5.1->d2l) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.2.4->d2l) (2022.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.25.1->d2l) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib==3.5.1->d2l) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7->matplotlib==3.5.1->d2l) (1.15.0)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel->jupyter==1.0.0->d2l) (5.3.5)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (2.6.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.8.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (57.4.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (1.0.18)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0->ipykernel->jupyter==1.0.0->d2l) (0.2.5)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (3.6.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (1.1.0)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (0.2.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets->jupyter==1.0.0->d2l) (5.4.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.10.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (2.15.3)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.3.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (4.11.4)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (21.4.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (5.7.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.2.0->ipywidgets->jupyter==1.0.0->d2l) (3.8.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (1.8.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (2.11.3)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook->jupyter==1.0.0->d2l) (0.13.3)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel->jupyter==1.0.0->d2l) (23.1.0)\n",
            "Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.1->notebook->jupyter==1.0.0->d2l) (0.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->notebook->jupyter==1.0.0->d2l) (2.0.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (5.0.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.7.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->jupyter==1.0.0->d2l) (1.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->jupyter==1.0.0->d2l) (0.5.1)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from qtconsole->jupyter==1.0.0->d2l) (2.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install d2l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from IPython import display\n",
        "from d2l import tensorflow as d2l\n",
        "\n",
        "batch_size = 256\n",
        "train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)"
      ],
      "metadata": {
        "id": "cnV2WU_0GS4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.1. Initializing Model Parameters**\n",
        "\n",
        "As in our linear regression example, each example here will be represented by a fixed-length vector. Each example in the raw dataset is a image. In this section, we will flatten each image, treating them as vectors of length 784. In the future, we will talk about more sophisticated strategies for exploiting the spatial structure in images, but for now we treat each pixel location as just another feature."
      ],
      "metadata": {
        "id": "8P2AWuYsGYfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "\n",
        "W = tf.Variable(tf.random.normal(shape=(num_inputs, num_outputs),\n",
        "                                 mean=0, stddev=0.01))\n",
        "b = tf.Variable(tf.zeros(num_outputs))"
      ],
      "metadata": {
        "id": "Cfish_xWGaxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.2. Defining the Softmax Operation**\n",
        "\n",
        "Before implementing the softmax regression model, let us briefly review how the sum operator works along specific dimensions in a tensor, as discussed in Section 2.3.6 and Section 2.3.6.1. Given a matrix X we can sum over all elements (by default) or only over elements in the same axis, i.e., the same column (axis 0) or the same row (axis 1). Note that if X is a tensor with shape (2, 3) and we sum over the columns, the result will be a vector with shape (3,). When invoking the sum operator, we can specify to keep the number of axes in the original tensor, rather than collapsing out the dimension that we summed over. This will result in a two-dimensional tensor with shape (1, 3)."
      ],
      "metadata": {
        "id": "xRjyUX5zGe8l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
        "tf.reduce_sum(X, 0, keepdims=True), tf.reduce_sum(X, 1, keepdims=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FykVWmEGiUQ",
        "outputId": "43d2888f-2843-4c8b-d62e-ca816018248d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(1, 3), dtype=float32, numpy=array([[5., 7., 9.]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(2, 1), dtype=float32, numpy=\n",
              " array([[ 6.],\n",
              "        [15.]], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(X):\n",
        "    X_exp = tf.exp(X)\n",
        "    partition = tf.reduce_sum(X_exp, 1, keepdims=True)\n",
        "    return X_exp / partition  # The broadcasting mechanism is applied here"
      ],
      "metadata": {
        "id": "LPdCp8Y8Gjiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.random.normal((2, 5), 0, 1)\n",
        "X_prob = softmax(X)\n",
        "X_prob, tf.reduce_sum(X_prob, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgOG0E3WGl0k",
        "outputId": "464abf3b-f2a7-4d1a-981e-28fcfd2dec8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(2, 5), dtype=float32, numpy=\n",
              " array([[0.13013774, 0.1487709 , 0.11898651, 0.26574194, 0.3363629 ],\n",
              "        [0.28079292, 0.19585279, 0.23189908, 0.2043122 , 0.08714298]],\n",
              "       dtype=float32)>,\n",
              " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.3. Defining the Model**\n",
        "\n",
        "Now that we have defined the softmax operation, we can implement the softmax regression model. The below code defines how the input is mapped to the output through the network. Note that we flatten each original image in the batch into a vector using the reshape function before passing the data through our model."
      ],
      "metadata": {
        "id": "kd3b-n-EGm29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def net(X):\n",
        "    return softmax(tf.matmul(tf.reshape(X, (-1, W.shape[0])), W) + b)"
      ],
      "metadata": {
        "id": "XonEbmRYGqRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.4. Defining the Loss Function**\n",
        "\n",
        "Next, we need to implement the cross-entropy loss function, as introduced in Section 3.4. This may be the most common loss function in all of deep learning because, at the moment, classification problems far outnumber regression problems."
      ],
      "metadata": {
        "id": "QXqK-2khGvbR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_hat = tf.constant([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
        "y = tf.constant([0, 2])\n",
        "tf.boolean_mask(y_hat, tf.one_hot(y, depth=y_hat.shape[-1]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndH_Fa1IGxx6",
        "outputId": "e5054667-ba2e-4bd1-f4c7-c2f67cd4e065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.1, 0.5], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_entropy(y_hat, y):\n",
        "    return -tf.math.log(tf.boolean_mask(\n",
        "        y_hat, tf.one_hot(y, depth=y_hat.shape[-1])))\n",
        "\n",
        "cross_entropy(y_hat, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOoORGo4G1Bt",
        "outputId": "fb5a1cbd-8833-49a0-b58a-a7d292c7bc7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2.3025851, 0.6931472], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.5. Classification Accuracy**\n",
        "\n",
        "Given the predicted probability distribution y_hat, we typically choose the class with the highest predicted probability whenever we must output a hard prediction. Indeed, many applications require that we make a choice. Gmail must categorize an email into “Primary”, “Social”, “Updates”, or “Forums”. It might estimate probabilities internally, but at the end of the day it has to choose one among the classes."
      ],
      "metadata": {
        "id": "x7NpdbmSG2Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_hat, y):  \n",
        "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
        "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
        "        y_hat = tf.argmax(y_hat, axis=1)\n",
        "    cmp = tf.cast(y_hat, y.dtype) == y\n",
        "    return float(tf.reduce_sum(tf.cast(cmp, y.dtype)))"
      ],
      "metadata": {
        "id": "c7-uQRrnG4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(y_hat, y) / len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3ysSpdIG7CK",
        "outputId": "3aa7987f-4674-4b37-e201-8c1faa8d12c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_accuracy(net, data_iter): \n",
        "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
        "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
        "    for X, y in data_iter:\n",
        "        metric.add(accuracy(net(X), y), d2l.size(y))\n",
        "    return metric[0] / metric[1]"
      ],
      "metadata": {
        "id": "X581dTElG_Au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Accumulator: \n",
        "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
        "    def __init__(self, n):\n",
        "        self.data = [0.0] * n\n",
        "\n",
        "    def add(self, *args):\n",
        "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
        "\n",
        "    def reset(self):\n",
        "        self.data = [0.0] * len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "wulj2CHHHATX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_accuracy(net, test_iter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIX59RcWHC_d",
        "outputId": "f4c6931f-e8f8-4741-b34e-8bd528acb754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1186"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.6. Training**\n",
        "\n",
        "The training loop for softmax regression should look strikingly familiar if you read through our implementation of linear regression in Section 3.2. Here we refactor the implementation to make it reusable. First, we define a function to train for one epoch. Note that updater is a general function to update the model parameters, which accepts the batch size as an argument. It can be either a wrapper of the d2l.sgd function or a framework’s built-in optimization function."
      ],
      "metadata": {
        "id": "3F-tLqQoHFku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_ch3(net, train_iter, loss, updater): \n",
        "    \"\"\"The training loop defined in Chapter 3.\"\"\"\n",
        "    # Sum of training loss, sum of training accuracy, no. of examples\n",
        "    metric = Accumulator(3)\n",
        "    for X, y in train_iter:\n",
        "        # Compute gradients and update parameters\n",
        "        with tf.GradientTape() as tape:\n",
        "            y_hat = net(X)\n",
        "            # Keras implementations for loss takes (labels, predictions)\n",
        "            # instead of (predictions, labels) that users might implement\n",
        "            # in this book, e.g. `cross_entropy` that we implemented above\n",
        "            if isinstance(loss, tf.keras.losses.Loss):\n",
        "                l = loss(y, y_hat)\n",
        "            else:\n",
        "                l = loss(y_hat, y)\n",
        "        if isinstance(updater, tf.keras.optimizers.Optimizer):\n",
        "            params = net.trainable_variables\n",
        "            grads = tape.gradient(l, params)\n",
        "            updater.apply_gradients(zip(grads, params))\n",
        "        else:\n",
        "            updater(X.shape[0], tape.gradient(l, updater.params))\n",
        "        # Keras loss by default returns the average loss in a batch\n",
        "        l_sum = l * float(tf.size(y)) if isinstance(\n",
        "            loss, tf.keras.losses.Loss) else tf.reduce_sum(l)\n",
        "        metric.add(l_sum, accuracy(y_hat, y), tf.size(y))\n",
        "    # Return training loss and training accuracy\n",
        "    return metric[0] / metric[2], metric[1] / metric[2]"
      ],
      "metadata": {
        "id": "x3jaST-hHHJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Animator:  \n",
        "    \"\"\"For plotting data in animation.\"\"\"\n",
        "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "                 ylim=None, xscale='linear', yscale='linear',\n",
        "                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,\n",
        "                 figsize=(3.5, 2.5)):\n",
        "        # Incrementally plot multiple lines\n",
        "        if legend is None:\n",
        "            legend = []\n",
        "        d2l.use_svg_display()\n",
        "        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)\n",
        "        if nrows * ncols == 1:\n",
        "            self.axes = [self.axes, ]\n",
        "        # Use a lambda function to capture arguments\n",
        "        self.config_axes = lambda: d2l.set_axes(\n",
        "            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)\n",
        "        self.X, self.Y, self.fmts = None, None, fmts\n",
        "\n",
        "    def add(self, x, y):\n",
        "        # Add multiple data points into the figure\n",
        "        if not hasattr(y, \"__len__\"):\n",
        "            y = [y]\n",
        "        n = len(y)\n",
        "        if not hasattr(x, \"__len__\"):\n",
        "            x = [x] * n\n",
        "        if not self.X:\n",
        "            self.X = [[] for _ in range(n)]\n",
        "        if not self.Y:\n",
        "            self.Y = [[] for _ in range(n)]\n",
        "        for i, (a, b) in enumerate(zip(x, y)):\n",
        "            if a is not None and b is not None:\n",
        "                self.X[i].append(a)\n",
        "                self.Y[i].append(b)\n",
        "        self.axes[0].cla()\n",
        "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
        "            self.axes[0].plot(x, y, fmt)\n",
        "        self.config_axes()\n",
        "        display.display(self.fig)\n",
        "        display.clear_output(wait=True)"
      ],
      "metadata": {
        "id": "0UQ2tuQ5HJ0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  \n",
        "    \"\"\"Train a model (defined in Chapter 3).\"\"\"\n",
        "    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
        "                        legend=['train loss', 'train acc', 'test acc'])\n",
        "    for epoch in range(num_epochs):\n",
        "        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)\n",
        "        test_acc = evaluate_accuracy(net, test_iter)\n",
        "        animator.add(epoch + 1, train_metrics + (test_acc,))\n",
        "    train_loss, train_acc = train_metrics\n",
        "    assert train_loss < 0.5, train_loss\n",
        "    assert train_acc <= 1 and train_acc > 0.7, train_acc\n",
        "    assert test_acc <= 1 and test_acc > 0.7, test_acc"
      ],
      "metadata": {
        "id": "p3dA_EG3HMNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Updater():  \n",
        "    \"\"\"For updating parameters using minibatch stochastic gradient descent.\"\"\"\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "\n",
        "    def __call__(self, batch_size, grads):\n",
        "        d2l.sgd(self.params, grads, self.lr, batch_size)\n",
        "\n",
        "updater = Updater([W, b], lr=0.1)\n",
        "num_epochs = 10"
      ],
      "metadata": {
        "id": "LZ9KPlMiHOuQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.7. Prediction**\n",
        "\n",
        "Now that training is complete, our model is ready to classify some images. Given a series of images, we will compare their actual labels (first line of text output) and the predictions from the model (second line of text output)."
      ],
      "metadata": {
        "id": "hEXrSPtbHUH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.6.8. Summary**\n",
        "\n",
        "With softmax regression, we can train models for multiclass classification.\n",
        "\n",
        "The training loop of softmax regression is very similar to that in linear regression: retrieve and read data, define models and loss functions, then train models using optimization algorithms. As you will soon find out, most common deep learning models have similar training procedures."
      ],
      "metadata": {
        "id": "lw-vRFixHcZF"
      }
    }
  ]
}